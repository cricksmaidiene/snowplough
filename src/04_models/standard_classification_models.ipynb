{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5b3662f-3f35-47a4-80e5-df6782110de7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Standard Classification Models - Topics & Authors ✏️\n",
    "\n",
    "This notebook uses non-neural network models to evaluate baselines and test preliminary issues, if any\n",
    "\n",
    "#### Notebook Properties\n",
    "* Upstream Notebook: `src.engineering.topic_processor`\n",
    "* Compute Resources: `64 GB RAM, 4 CPUs`\n",
    "* Last Updated: `Dec 4 2023`\n",
    "\n",
    "#### Data\n",
    "\n",
    "| **Name** | **Type** | **Location Type** | **Description** | **Location** | \n",
    "| --- | --- | --- | --- | --- | \n",
    "| `all_the_news` | `input` | `Delta` | Read full delta dataset of `AllTheNews` | `catalog/simple_topic/all_the_news.delta` | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a9a3a19-3245-4e37-8703-b043dc4898b2",
     "showTitle": true,
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from typing import Any, Callable\n",
    "from loguru import logger\n",
    "\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "from deltalake import DeltaTable\n",
    "from tqdm.autonotebook import tqdm\n",
    "from src.utils.io import FileSystemHandler\n",
    "from src.utils.functions import all_stopwords\n",
    "\n",
    "import nltk\n",
    "import mlflow\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "505ade55-eb60-4fca-9843-2d715c796ee5",
     "showTitle": true,
     "title": "Settings"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_info_rows\", 10_000_000)\n",
    "pd.set_option(\"display.max_info_columns\", 1_000)\n",
    "\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "tqdm.pandas()\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('omw-1.4')\n",
    "datafs = FileSystemHandler(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ee3d328-dbcb-4223-8cd7-16e9ef664808",
     "showTitle": true,
     "title": "Input Parameters"
    }
   },
   "outputs": [],
   "source": [
    "LIMIT_PARTITIONS: int | None = None\n",
    "\"\"\"An input parameter to limit the number of table partitions to read from delta. Useful to perform EDA on a sample of data.\"\"\"\n",
    "\n",
    "SHUFFLE_PARTITIONS: bool = False\n",
    "\"\"\"Whether to randomize the partitions before reading\"\"\"\n",
    "\n",
    "INPUT_TABLE: str = \"all_the_news\" \n",
    "INPUT_CATALOG: str = \"simple_topic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eabd5bff-d0ce-4d17-a164-501618213e4b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8fb0b6c-9fbe-419e-a77a-1877401f350c",
     "showTitle": true,
     "title": "Read Input Data"
    }
   },
   "outputs": [],
   "source": [
    "atn_delta_table: DeltaTable = datafs.read_delta(\n",
    "    table=INPUT_TABLE,\n",
    "    catalog_name=INPUT_CATALOG,\n",
    "    as_pandas=False,\n",
    ")\n",
    "\n",
    "df: pd.DataFrame = datafs.read_delta_partitions(\n",
    "    delta_table=atn_delta_table,\n",
    "    N_partitions=LIMIT_PARTITIONS,\n",
    "    shuffle_partitions=SHUFFLE_PARTITIONS,\n",
    ")\n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df = df.sort_values(by=[\"date\"])\n",
    "\n",
    "df = df[~df.is_geo]\n",
    "df = df[[\"date\", \"publication\", \"author\", \"title\", \"article\", \"section\", \"simple_topic\"]]\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2942ebbf-ae0b-47b7-a6f0-204e02d11735",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ded1792-4bb3-4659-a61d-5bcedbdb59e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Basic Preprocessing\n",
    "\n",
    "* Filtering Rows to a Single Year\n",
    "* Cleaning up stopwords, lemmatization, case normalization and other tweaks to articles and titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ef61c1c-5d8d-4eee-a2e3-7acf7ea2eac2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_y = df[df.date.dt.year == 2019]\n",
    "print(df_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f567c2ed-ce61-459c-9e73-f5a3b43fc2e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocess_text(x: str) -> str:\n",
    "    x = x.lower()\n",
    "    x = re.sub(r\"\\W\", \" \", x)\n",
    "    x = re.sub(r\"\\s+\", \" \", x)\n",
    "    y = x.split()\n",
    "    y = [word for word in y if word not in all_stopwords]\n",
    "    y = [lemmatizer.lemmatize(word) for word in y]\n",
    "    return \" \".join(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bca242b-1736-4649-a506-d853f3318d7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_y[\"title_clean\"] = df_y[\"title\"].dropna().apply(preprocess_text)\n",
    "df_y[[\"title\", \"title_clean\"]].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "076ee3de-9a01-43de-a19f-2ffac4de81f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_y[\"article_clean\"] = df_y[\"article\"].dropna().progress_apply(preprocess_text)\n",
    "df_y[[\"article\", \"article_clean\"]].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "422c1d18-59ff-45a1-ba01-32ed6938b179",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_df = df_y[df_y.simple_topic != \"Commercial Business\"]\n",
    "sample_df = (\n",
    "    sample_df.dropna(subset=[\"title_clean\"])\n",
    "    .dropna(subset=[\"article_clean\"])\n",
    "    .replace([np.nan], [None])\n",
    ")\n",
    "print(sample_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1d5fdef-3eff-4976-93c3-ddd25a3c1452",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "BASE_DIR_NAME: str = \"experiment_results\"\n",
    "NOTEBOOK_DIR_NAME: str = \"standard_classification_models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e2b8435-02fb-4d10-a7b6-5453136d383d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Declare Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17c0fe9e-582f-4733-b50f-e2aeeeb50a74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "baseline_model_callables: dict[str, Callable] = dict(\n",
    "    logistic_regression=LogisticRegression,\n",
    "    naive_bayes=MultinomialNB,\n",
    "    random_forest=RandomForestClassifier,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e999fb33-3593-427c-ba36-ed1a0677848f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_generic_model(model_classifier: Any, X_train, y_train, X_test,) -> Any:\n",
    "    model_classifier.fit(X_train, y_train)\n",
    "    model_predictions = model_classifier.predict(X_test)\n",
    "    return model_predictions\n",
    "\n",
    "\n",
    "def get_trained_model_stats(\n",
    "    y_test, model_predictions\n",
    ") -> tuple[float, float, float, float]:\n",
    "    accuracy = accuracy_score(y_test, model_predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_test, model_predictions, average=\"weighted\"\n",
    "    )\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "def get_train_and_test_data(\n",
    "    vectorizer,\n",
    "    dataframe: pd.DataFrame,\n",
    "    input_col: str,\n",
    "    target_col: str,\n",
    "    test_frac: float = 0.2,\n",
    "):\n",
    "    input_features = vectorizer.fit_transform(dataframe[input_col])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        input_features, dataframe[target_col], test_size=test_frac, random_state=50\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1d58477-8514-413d-aee1-8bfd5264696f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Title -> Topic Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "607eb3ca-6619-4eeb-bd89-c7b94063939d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TITLE_TOPIC_TF_IDF_MAX_FEAT: int = 50_000\n",
    "TITLE_TOPIC_TFIDF_EXP_NAME: str = \"title_topic_tfidf\"\n",
    "\n",
    "title_topic_tfidf_baseline_results: dict = {}\n",
    "title_topic_tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=TITLE_TOPIC_TF_IDF_MAX_FEAT,\n",
    ")\n",
    "\n",
    "title_topic_tfidf_dir_name: str = (\n",
    "    f\"./{BASE_DIR_NAME}\"\n",
    "    + f\"/{NOTEBOOK_DIR_NAME}\"\n",
    "    + f\"/{TITLE_TOPIC_TFIDF_EXP_NAME}\"\n",
    "    + f\"_{TITLE_TOPIC_TF_IDF_MAX_FEAT}\"\n",
    ")\n",
    "os.makedirs(title_topic_tfidf_dir_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa2dee3e-db38-4445-9f0b-cc3b9acc3247",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    X_title_topic_tfidf_train,\n",
    "    X_title_topic_tfidf_test,\n",
    "    y_title_topic_tfidf_train,\n",
    "    y_title_topic_tfidf_test,\n",
    ") = get_train_and_test_data(\n",
    "    title_topic_tfidf_vectorizer,\n",
    "    sample_df,\n",
    "    \"title_clean\",\n",
    "    \"simple_topic\",\n",
    ")\n",
    "\n",
    "for model_name, model_callable in baseline_model_callables.items():\n",
    "    logger.info(f\"Training: {model_name}\")\n",
    "    classifier_instance = model_callable()\n",
    "    classifier_predictions = train_generic_model(\n",
    "        model_classifier=classifier_instance,\n",
    "        X_train=X_title_topic_tfidf_train,\n",
    "        y_train=y_title_topic_tfidf_train,\n",
    "        X_test=X_title_topic_tfidf_test,\n",
    "    )\n",
    "    title_topic_tfidf_baseline_results[model_name] = classifier_predictions\n",
    "\n",
    "    model_str: str = classification_report(\n",
    "        y_title_topic_tfidf_test, classifier_predictions\n",
    "    )\n",
    "    with open(f\"{title_topic_tfidf_dir_name}/{model_name}.txt\", \"w\") as f:\n",
    "        f.write(model_str)\n",
    "\n",
    "    logger.info(f\"Complete: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72c8deb2-b9cb-474a-8f2d-ad4187cbd224",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Article -> Topic Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a023d323-20f9-4260-9174-b278e0e8cfe8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ARTICLE_TOPIC_TF_IDF_MAX_FEAT: int = 10_000\n",
    "ARTICLE_TOPIC_TFIDF_EXP_NAME: str = \"article_topic_tfidf\"\n",
    "\n",
    "article_topic_tfidf_baseline_results: dict = {}\n",
    "article_topic_tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=ARTICLE_TOPIC_TF_IDF_MAX_FEAT,\n",
    ")\n",
    "\n",
    "article_topic_tfidf_dir_name: str = (\n",
    "    f\"./{BASE_DIR_NAME}\"\n",
    "    + f\"/{NOTEBOOK_DIR_NAME}\"\n",
    "    + f\"/{ARTICLE_TOPIC_TFIDF_EXP_NAME}\"\n",
    "    + f\"_{ARTICLE_TOPIC_TF_IDF_MAX_FEAT}\"\n",
    ")\n",
    "os.makedirs(article_topic_tfidf_dir_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c4d6ba6-1dcd-4ef6-9fa4-b48b43f27266",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    X_article_topic_tfidf_train,\n",
    "    X_article_topic_tfidf_test,\n",
    "    y_article_topic_tfidf_train,\n",
    "    y_article_topic_tfidf_test,\n",
    ") = get_train_and_test_data(\n",
    "    article_topic_tfidf_vectorizer,\n",
    "    sample_df,\n",
    "    \"article_clean\",\n",
    "    \"simple_topic\",\n",
    ")\n",
    "\n",
    "for model_name, model_callable in baseline_model_callables.items():\n",
    "    logger.info(f\"Training: {model_name}\")\n",
    "    classifier_instance = model_callable()\n",
    "    classifier_predictions = train_generic_model(\n",
    "        model_classifier=classifier_instance,\n",
    "        X_train=X_article_topic_tfidf_train,\n",
    "        y_train=y_article_topic_tfidf_train,\n",
    "        X_test=X_article_topic_tfidf_test,\n",
    "    )\n",
    "    article_topic_tfidf_baseline_results[model_name] = classifier_predictions\n",
    "\n",
    "    model_str: str = classification_report(\n",
    "        y_article_topic_tfidf_test, classifier_predictions\n",
    "    )\n",
    "    with open(f\"{article_topic_tfidf_dir_name}/{model_name}.txt\", \"w\") as f:\n",
    "        f.write(model_str)\n",
    "\n",
    "    logger.info(f\"Complete: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc6b7af1-2608-4299-90c1-c9bfe64ccd89",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Title -> Author Classifiers\n",
    "\n",
    "> We don't worry about `author -> topic` classifier since the topics that certain authors write about should automatically be encoded within the articles to authors classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31233035-b7a6-40c4-9966-f14c2dd37df5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Author Labels Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a221a25-203a-4676-94a0-76aade0de9c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "author_article_threshold: int = 100\n",
    "\n",
    "unique_authors = df_y[\"author\"].dropna().value_counts()\n",
    "unique_authors = unique_authors[\n",
    "    (unique_authors > author_article_threshold)\n",
    "    & ~(\n",
    "        (unique_authors.index.str.contains(\"staff\", case=False))\n",
    "        | (unique_authors.index.str.contains(\"media\", case=False))\n",
    "        | (unique_authors.index.str.contains(\"network\", case=False))\n",
    "        | (unique_authors.index.str.contains(\"press\", case=False))\n",
    "    )\n",
    "]\n",
    "\n",
    "unique_authors = unique_authors.to_frame().reset_index()\n",
    "unique_authors.columns = [\"author\", \"article_count\"]\n",
    "\n",
    "unique_authors = (\n",
    "    unique_authors[\n",
    "        unique_authors.author.apply(\n",
    "            lambda cell: not any(\n",
    "                [p.lower() in cell.lower() for p in df_y.publication.unique()]\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "    .reset_index(drop=True)\n",
    "    .drop_duplicates(subset=[\"author\"])\n",
    ")\n",
    "\n",
    "print(unique_authors.shape)\n",
    "unique_authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a3685b1-c059-4107-9263-2d31600363e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "author_article_df: pd.DataFrame = (\n",
    "    df_y[\n",
    "        (df_y.author.isin(unique_authors.author))\n",
    "        & (df_y.simple_topic != \"Commercial Business\")\n",
    "    ]\n",
    "    .dropna(subset=[\"author\"])\n",
    "    .dropna(subset=[\"article_clean\"])\n",
    "    .dropna(subset=[\"title_clean\"])\n",
    ")\n",
    "\n",
    "print(author_article_df.shape)\n",
    "author_article_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb646799-cb79-4334-bca1-52a39901cc9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TITLE_AUTHOR_TF_IDF_MAX_FEAT: int = 10_000\n",
    "TITLE_AUTHOR_TFIDF_EXP_NAME: str = \"title_author_tfidf\"\n",
    "\n",
    "title_author_tfidf_baseline_results: dict = {}\n",
    "title_author_tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=TITLE_AUTHOR_TF_IDF_MAX_FEAT,\n",
    ")\n",
    "\n",
    "title_author_tfidf_dir_name: str = (\n",
    "    f\"./{BASE_DIR_NAME}\"\n",
    "    + f\"/{NOTEBOOK_DIR_NAME}\"\n",
    "    + f\"/{TITLE_AUTHOR_TFIDF_EXP_NAME}\"\n",
    "    + f\"_{TITLE_AUTHOR_TF_IDF_MAX_FEAT}\"\n",
    ")\n",
    "os.makedirs(title_author_tfidf_dir_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8b6878b-f2a5-4dc8-93ff-70ecfc2b9e7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    X_title_author_tfidf_train,\n",
    "    X_title_author_tfidf_test,\n",
    "    y_title_author_tfidf_train,\n",
    "    y_title_author_tfidf_test,\n",
    ") = get_train_and_test_data(\n",
    "    title_author_tfidf_vectorizer,\n",
    "    author_article_df,\n",
    "    \"title_clean\",\n",
    "    \"author\",\n",
    ")\n",
    "\n",
    "for model_name, model_callable in baseline_model_callables.items():\n",
    "    logger.info(f\"Training: {model_name}\")\n",
    "    classifier_instance = model_callable()\n",
    "    classifier_predictions = train_generic_model(\n",
    "        model_classifier=classifier_instance,\n",
    "        X_train=X_title_author_tfidf_train,\n",
    "        y_train=y_title_author_tfidf_train,\n",
    "        X_test=X_title_author_tfidf_test,\n",
    "    )\n",
    "    title_author_tfidf_baseline_results[model_name] = classifier_predictions\n",
    "\n",
    "    model_str: str = classification_report(\n",
    "        y_title_author_tfidf_test, classifier_predictions\n",
    "    )\n",
    "    with open(f\"{title_author_tfidf_dir_name}/{model_name}.txt\", \"w\") as f:\n",
    "        f.write(model_str)\n",
    "\n",
    "    logger.info(f\"Complete: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "346700e6-40c6-45e8-9530-6ae195ded3f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Article -> Author Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b554c7ba-47bd-4dcd-961f-0b4d501b73fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ARTICLE_AUTHOR_TF_IDF_MAX_FEAT: int = 5_000\n",
    "ARTICLE_AUTHOR_TFIDF_EXP_NAME: str = \"article_author_tfidf\"\n",
    "\n",
    "article_author_tfidf_baseline_results: dict = {}\n",
    "article_author_tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=ARTICLE_AUTHOR_TF_IDF_MAX_FEAT,\n",
    ")\n",
    "\n",
    "article_author_tfidf_dir_name: str = (\n",
    "    f\"./{BASE_DIR_NAME}\"\n",
    "    + f\"/{NOTEBOOK_DIR_NAME}\"\n",
    "    + f\"/{ARTICLE_AUTHOR_TFIDF_EXP_NAME}\"\n",
    "    + f\"_{ARTICLE_AUTHOR_TF_IDF_MAX_FEAT}\"\n",
    ")\n",
    "os.makedirs(article_author_tfidf_dir_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fe42caa-603a-4ba5-9549-abadbcb423d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    X_article_author_tfidf_train,\n",
    "    X_article_author_tfidf_test,\n",
    "    y_article_author_tfidf_train,\n",
    "    y_article_author_tfidf_test,\n",
    ") = get_train_and_test_data(\n",
    "    article_author_tfidf_vectorizer,\n",
    "    author_article_df,\n",
    "    \"article_clean\",\n",
    "    \"author\",\n",
    ")\n",
    "\n",
    "for model_name, model_callable in baseline_model_callables.items():\n",
    "    logger.info(f\"Training: {model_name}\")\n",
    "    classifier_instance = model_callable()\n",
    "    classifier_predictions = train_generic_model(\n",
    "        model_classifier=classifier_instance,\n",
    "        X_train=X_article_author_tfidf_train,\n",
    "        y_train=y_article_author_tfidf_train,\n",
    "        X_test=X_article_author_tfidf_test,\n",
    "    )\n",
    "    article_author_tfidf_baseline_results[model_name] = classifier_predictions\n",
    "\n",
    "    model_str: str = classification_report(\n",
    "        y_article_author_tfidf_test, classifier_predictions\n",
    "    )\n",
    "    with open(f\"{article_author_tfidf_dir_name}/{model_name}.txt\", \"w\") as f:\n",
    "        f.write(model_str)\n",
    "\n",
    "    logger.info(f\"Complete: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20146149-0238-48f5-ac60-a623167dce66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "standard_classification_models",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
