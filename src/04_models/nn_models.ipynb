{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee92ff99-6989-40e4-9119-fbc3c898a9d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Neural Network Models - Topics ðŸ§ \n",
    "\n",
    "This notebook uses simple (non-transformer) neural network models to train the topic classifier\n",
    "\n",
    "#### Notebook Properties\n",
    "* Upstream Notebook: `src.engineering.topic_processor`\n",
    "* Compute Resources: `32 GB RAM, 1 GPU` (maybe?)\n",
    "* Last Updated: `Dec 10 2023`\n",
    "\n",
    "#### Data\n",
    "\n",
    "| **Name** | **Type** | **Location Type** | **Description** | **Location** | \n",
    "| --- | --- | --- | --- | --- | \n",
    "| `all_the_news` | `input` | `Delta` | Read full delta dataset of `AllTheNews` | `catalog/simple_topic/all_the_news.delta` | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c96aa4a5-a19f-45b8-9264-e0bfc92a998a",
     "showTitle": true,
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "from typing import Any, Callable\n",
    "from loguru import logger\n",
    "import random\n",
    "\n",
    "from deltalake import DeltaTable\n",
    "from tqdm.autonotebook import tqdm\n",
    "from src.utils.io import FileSystemHandler\n",
    "from src.utils.functions import all_stopwords\n",
    "\n",
    "import nltk\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional,Conv1D,GlobalMaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af838e60-c6bf-4b20-81ac-c5d7f94908a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "# Setup strategy based on the available device: GPU or CPU\n",
    "if gpus:\n",
    "    try:\n",
    "        # If GPUs are available, use MirroredStrategy for distributed training\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)  # Optional: Enable memory growth\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        print(\"Running on GPU:\", gpus[0])\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    # If no GPUs are available, use the default strategy that works on CPU and single GPU\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc4f30c4-5669-44f2-8030-bb14681eb37a",
     "showTitle": true,
     "title": "Settings"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_info_rows\", 10_000_000)\n",
    "\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "tqdm.pandas()\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('omw-1.4')\n",
    "datafs = FileSystemHandler(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5acef12e-cee0-48dc-9d29-c663289f37f4",
     "showTitle": true,
     "title": "Input Parameters"
    }
   },
   "outputs": [],
   "source": [
    "LIMIT_PARTITIONS: int | None = None\n",
    "\"\"\"An input parameter to limit the number of table partitions to read from delta. Useful to perform EDA on a sample of data.\"\"\"\n",
    "\n",
    "SHUFFLE_PARTITIONS: bool = False\n",
    "\"\"\"Whether to randomize the partitions before reading\"\"\"\n",
    "\n",
    "INPUT_TABLE: str = \"all_the_news\" \n",
    "INPUT_CATALOG: str = \"simple_topic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "637e053a-2959-4bc3-bc44-9adb4ad30e68",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09250a5e-bb5d-496e-9e08-55f4b30a963e",
     "showTitle": true,
     "title": "Read Input Data"
    }
   },
   "outputs": [],
   "source": [
    "atn_delta_table: DeltaTable = datafs.read_delta(\n",
    "    table=INPUT_TABLE,\n",
    "    catalog_name=INPUT_CATALOG,\n",
    "    as_pandas=False,\n",
    ")\n",
    "\n",
    "df: pd.DataFrame = datafs.read_delta_partitions(\n",
    "    delta_table=atn_delta_table,\n",
    "    N_partitions=LIMIT_PARTITIONS,\n",
    "    shuffle_partitions=SHUFFLE_PARTITIONS,\n",
    ")\n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df = df.sort_values(by=[\"date\"])\n",
    "\n",
    "df = df[[\"date\", \"publication\", \"author\", \"title\", \"article\", \"section\", \"simple_topic\"]]\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "945d90df-cbdd-4aa0-affb-c1464bd29340",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e6c6937-35aa-49e1-987c-f3cb6340ea4f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Basic Preprocessing\n",
    "\n",
    "* Filtering Rows to a Single Year\n",
    "* Cleaning up stopwords, lemmatization, case normalization and other tweaks to articles and titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d43114b7-c356-4a89-b969-c7907eae59c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_y = df[df.date.dt.year == 2019]\n",
    "# df_y = df.copy()\n",
    "print(df_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b17652a0-8004-49c7-aebd-84f79ddc1cbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocess_text(x: str) -> str:\n",
    "    x = x.lower()\n",
    "    x = re.sub(r\"\\W\", \" \", x)\n",
    "    x = re.sub(r\"\\s+\", \" \", x)\n",
    "    y = x.split()\n",
    "    y = [word for word in y if word not in all_stopwords]\n",
    "    y = [lemmatizer.lemmatize(word) for word in y]\n",
    "    return \" \".join(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d702752d-fc8e-4f23-88d5-af191be73db2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_y[\"title_clean\"] = df_y[\"title\"].dropna().progress_apply(preprocess_text)\n",
    "df_y[[\"title\", \"title_clean\"]].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95d2dddc-ac67-4820-93a0-e2d734044466",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_y[\"article_clean\"] = df_y[\"article\"].dropna().progress_apply(preprocess_text)\n",
    "df_y[[\"article\", \"article_clean\"]].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95eb6167-6535-4c9d-b7e6-e4fabb326a72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "topic_to_id = {topic: id for id, topic in enumerate(df[\"simple_topic\"].unique())}\n",
    "id_to_topic = {id: topic for topic, id in topic_to_id.items()}\n",
    "\n",
    "df_y[\"simple_topic_id\"] = df_y[\"simple_topic\"].map(topic_to_id)\n",
    "df_y[[\"simple_topic\", \"simple_topic_id\"]].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80fe482f-f3d2-462b-9e2c-a4253a807591",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_y = df_y.dropna(subset=[\"article_clean\"]).dropna(subset=[\"simple_topic_id\"])\n",
    "print(df_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efecbcda-d1fb-4de8-afe7-b7d672dfadbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "BASE_DIR_NAME: str = \"experiment_results\"\n",
    "NOTEBOOK_DIR_NAME: str = \"nn_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0110152d-f612-4292-af37-9c7361c66a8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_bidirectional_lstm(\n",
    "    df: pd.DataFrame,\n",
    "    input_col: str,\n",
    "    target_col: str,\n",
    "    sample_size: int | None = None,\n",
    "    target_col_inverse_mapping: dict | None = None,\n",
    "    max_len: int = 32,\n",
    "    num_words: int = 10_000,\n",
    "    embedding_output_dim: int = 128,\n",
    "    lstm_allowed_units: list[int] = [32, 64, 128],\n",
    "    learning_rate: float = 0.001,\n",
    "    batch_size: int = 32,\n",
    "    epochs: int = 5,\n",
    "):\n",
    "    title_topic_dir_name: str = (\n",
    "        f\"./{BASE_DIR_NAME}\"\n",
    "        + f\"/{NOTEBOOK_DIR_NAME}\"\n",
    "        + f\"/bid_lstm_article\"\n",
    "        + f\"/{datetime.utcnow().strftime('%Y%m%d-%H%M')}\"\n",
    "    )\n",
    "    os.makedirs(title_topic_dir_name, exist_ok=True)\n",
    "\n",
    "    sample_size = sample_size if sample_size else len(df)\n",
    "    sample_df: pd.DataFrame = df.sample(sample_size)\n",
    "\n",
    "    input_values: np.ndarray = sample_df[input_col].values\n",
    "    target_values: np.ndarray = sample_df[target_col].values\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=num_words, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(input_values)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    sequences = tokenizer.texts_to_sequences(input_values)\n",
    "    padded_sequences = pad_sequences(sequences, padding=\"post\", maxlen=max_len)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        padded_sequences,\n",
    "        target_values,\n",
    "        test_size=0.2,\n",
    "        random_state=50,\n",
    "    )\n",
    "\n",
    "    lstm_1_units: int = random.choice(lstm_allowed_units)\n",
    "    lstm_2_units: int = random.choice(lstm_allowed_units)\n",
    "    dense_units: int = random.choice(lstm_allowed_units)\n",
    "\n",
    "    # lstm_1_units: int = 128\n",
    "    # lstm_2_units: int = 64\n",
    "    # dense_units: int = 32\n",
    "\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Embedding(\n",
    "                input_dim=num_words,\n",
    "                output_dim=embedding_output_dim,\n",
    "                input_length=max_len,\n",
    "            ),\n",
    "            Bidirectional(LSTM(units=lstm_1_units, return_sequences=True)),\n",
    "            Bidirectional(LSTM(units=lstm_2_units)),\n",
    "            Dense(dense_units, activation=\"relu\"),\n",
    "            Dropout(0.5),\n",
    "            Dense(len(np.unique(target_values)), activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    model_history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test, y_test),\n",
    "    )\n",
    "\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "\n",
    "    target_classification_report = classification_report(\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        target_names=[\n",
    "            target_col_inverse_mapping[i]\n",
    "            for i in sorted(set(y_test) | set(y_pred))\n",
    "            if i in target_col_inverse_mapping\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    with open(f\"{title_topic_dir_name}/classification_report.txt\", \"w\") as f:\n",
    "        f.write(target_classification_report)\n",
    "\n",
    "    output_params = dict(\n",
    "        max_len=max_len,\n",
    "        num_words=num_words,\n",
    "        embedding_output_dim=embedding_output_dim,\n",
    "        learning_rate=learning_rate,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        lstm_1_units=lstm_1_units,\n",
    "        lstm_2_units=lstm_2_units,\n",
    "        dense_units=dense_units,\n",
    "    )\n",
    "\n",
    "    with open(f\"{title_topic_dir_name}/hyperparameters.json\", \"w\") as f:\n",
    "        json.dump(output_params, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83b4204a-ab10-4be2-ba86-31a85439f50e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_bidirectional_lstm(\n",
    "    df=df_y,\n",
    "    input_col=\"article_clean\",\n",
    "    target_col=\"simple_topic_id\",\n",
    "    target_col_inverse_mapping=id_to_topic,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4402c3bc-e79e-474e-9f88-6e5c1c4e27d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_convolutional_neural_network(\n",
    "    df: pd.DataFrame,\n",
    "    input_col: str,\n",
    "    target_col: str,\n",
    "    sample_size: int | None = None,\n",
    "    target_col_inverse_mapping: dict | None = None,\n",
    "    max_len: int = 20,\n",
    "    num_words: int = 10_000,\n",
    "    embedding_output_dim: int = 64,\n",
    "    filter_sizes: list[int] = [64, 128],\n",
    "    kernel_size: int = 5,\n",
    "    learning_rate: float = 0.0001,\n",
    "    batch_size: int = 64,\n",
    "    epochs: int = 5,\n",
    "):\n",
    "    title_topic_dir_name: str = (\n",
    "        f\"./{BASE_DIR_NAME}\"\n",
    "        + f\"/{NOTEBOOK_DIR_NAME}\"\n",
    "        + f\"/cnn\"\n",
    "        + f\"/{datetime.utcnow().strftime('%Y%m%d-%H%M')}\"\n",
    "    )\n",
    "    os.makedirs(title_topic_dir_name, exist_ok=True)\n",
    "\n",
    "    sample_size = sample_size if sample_size else len(df)\n",
    "    sample_df: pd.DataFrame = df.sample(sample_size)\n",
    "\n",
    "    input_values: np.ndarray = sample_df[input_col].values\n",
    "    target_values: np.ndarray = sample_df[target_col].values\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=num_words, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(input_values)\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(input_values)\n",
    "    padded_sequences = pad_sequences(sequences, padding=\"post\", maxlen=max_len)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        padded_sequences,\n",
    "        target_values,\n",
    "        test_size=0.2,\n",
    "        random_state=50,\n",
    "    )\n",
    "\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Embedding(\n",
    "                input_dim=num_words,\n",
    "                output_dim=embedding_output_dim,\n",
    "                input_length=max_len,\n",
    "            ),\n",
    "            Conv1D(\n",
    "                filters=random.choice(filter_sizes),\n",
    "                kernel_size=kernel_size,\n",
    "                activation=\"relu\",\n",
    "            ),\n",
    "            GlobalMaxPooling1D(),\n",
    "            Dense(64, activation=\"relu\"),\n",
    "            Dropout(0.5),\n",
    "            Dense(len(np.unique(target_values)), activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    model_history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test, y_test),\n",
    "    )\n",
    "\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "\n",
    "    target_classification_report = classification_report(\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        target_names=[\n",
    "            target_col_inverse_mapping[i]\n",
    "            for i in sorted(set(y_test) | set(y_pred))\n",
    "            if i in target_col_inverse_mapping\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    with open(f\"{title_topic_dir_name}/classification_report.txt\", \"w\") as f:\n",
    "        f.write(target_classification_report)\n",
    "\n",
    "    output_params = dict(\n",
    "        max_len=max_len,\n",
    "        num_words=num_words,\n",
    "        embedding_output_dim=embedding_output_dim,\n",
    "        filter_sizes=filter_sizes,\n",
    "        kernel_size=kernel_size,\n",
    "        learning_rate=learning_rate,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "    )\n",
    "\n",
    "    with open(f\"{title_topic_dir_name}/hyperparameters.json\", \"w\") as f:\n",
    "        json.dump(output_params, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "159bb1e8-f02a-4378-b366-7f22639deb6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_convolutional_neural_network(\n",
    "    df=df_y,\n",
    "    input_col=\"article_clean\",\n",
    "    target_col=\"simple_topic_id\",\n",
    "    target_col_inverse_mapping=id_to_topic,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2df3c393-56e7-4685-b036-a4dc2a46a316",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nn_models",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
