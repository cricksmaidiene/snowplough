{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ad9bab2-0dfc-4e59-8c1b-8b1a34721050",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Fine-Tuning BERT - Topics ðŸ¤–\n",
    "\n",
    "This notebook fine-tunes a BERT model to classify news topics\n",
    "\n",
    "#### Notebook Properties\n",
    "* Upstream Notebook: `src.engineering.topic_processor`\n",
    "* Compute Resources: `61 GB RAM, 1 GPU` (maybe?)\n",
    "* Last Updated: `Dec 10 2023`\n",
    "\n",
    "#### Data\n",
    "\n",
    "| **Name** | **Type** | **Location Type** | **Description** | **Location** | \n",
    "| --- | --- | --- | --- | --- | \n",
    "| `all_the_news` | `input` | `Delta` | Read full delta dataset of `AllTheNews` | `catalog/simple_topic/all_the_news.delta` | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b911a3f3-bc88-412e-b400-1176c58f26f6",
     "showTitle": true,
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "from typing import Any, Callable\n",
    "from loguru import logger\n",
    "import random\n",
    "\n",
    "from deltalake import DeltaTable\n",
    "from tqdm.autonotebook import tqdm\n",
    "from src.utils.io import FileSystemHandler\n",
    "from src.utils.functions import all_stopwords\n",
    "\n",
    "import nltk\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, LayerNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling1D\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding,\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Bidirectional,\n",
    "    Conv1D,\n",
    "    GlobalMaxPooling1D,\n",
    ")\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a86ef66e-b0d4-4fbc-b565-66ece27c4c5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "# Setup strategy based on the available device: GPU or CPU\n",
    "if gpus:\n",
    "    try:\n",
    "        # If GPUs are available, use MirroredStrategy for distributed training\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)  # Optional: Enable memory growth\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        print(\"Running on GPU:\", gpus[0])\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    # If no GPUs are available, use the default strategy that works on CPU and single GPU\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7299606-5a6f-4481-83e5-12d6a3789b56",
     "showTitle": true,
     "title": "Settings"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_info_rows\", 10_000_000)\n",
    "\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "tqdm.pandas()\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('omw-1.4')\n",
    "datafs = FileSystemHandler(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1221106c-c685-443a-9d37-13c60ea3e2e4",
     "showTitle": true,
     "title": "Input Parameters"
    }
   },
   "outputs": [],
   "source": [
    "LIMIT_PARTITIONS: int | None = None\n",
    "\"\"\"An input parameter to limit the number of table partitions to read from delta. Useful to perform EDA on a sample of data.\"\"\"\n",
    "\n",
    "SHUFFLE_PARTITIONS: bool = False\n",
    "\"\"\"Whether to randomize the partitions before reading\"\"\"\n",
    "\n",
    "INPUT_TABLE: str = \"all_the_news\" \n",
    "INPUT_CATALOG: str = \"simple_topic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d884577f-e338-47c6-9ec6-b95e77f40324",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6af1fc1-c892-430c-8687-a5fbc9b3f759",
     "showTitle": true,
     "title": "Read Input Data"
    }
   },
   "outputs": [],
   "source": [
    "atn_delta_table: DeltaTable = datafs.read_delta(\n",
    "    table=INPUT_TABLE,\n",
    "    catalog_name=INPUT_CATALOG,\n",
    "    as_pandas=False,\n",
    ")\n",
    "\n",
    "df: pd.DataFrame = datafs.read_delta_partitions(\n",
    "    delta_table=atn_delta_table,\n",
    "    N_partitions=LIMIT_PARTITIONS,\n",
    "    shuffle_partitions=SHUFFLE_PARTITIONS,\n",
    ")\n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df = df.sort_values(by=[\"date\"])\n",
    "\n",
    "df = df[[\"date\", \"publication\", \"author\", \"title\", \"article\", \"section\", \"simple_topic\"]]\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4c5a2ff-4490-4c4b-a5cf-9b703c52e6d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa383ff2-149e-4b39-bdee-84b1d3de53e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Basic Preprocessing\n",
    "\n",
    "* Filtering Rows to a Single Year\n",
    "* Cleaning up stopwords, lemmatization, case normalization and other tweaks to articles and titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17c9970c-6440-4b77-b206-bb2655cb99b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_y = df[df.date.dt.year == 2019]\n",
    "# df_y = df.copy()\n",
    "print(df_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd41a593-951c-4b06-b0c9-89c4868c1078",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocess_text(x: str) -> str:\n",
    "    x = x.lower()\n",
    "    x = re.sub(r\"\\W\", \" \", x)\n",
    "    x = re.sub(r\"\\s+\", \" \", x)\n",
    "    y = x.split()\n",
    "    y = [word for word in y if word not in all_stopwords]\n",
    "    y = [lemmatizer.lemmatize(word) for word in y]\n",
    "    return \" \".join(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68f3c68e-99ff-4ea0-90f0-a97e161bb4cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_y[\"title_clean\"] = df_y[\"title\"].dropna().progress_apply(preprocess_text)\n",
    "df_y[[\"title\", \"title_clean\"]].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72d5d651-6a91-4c08-9686-7f146359f682",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_y[\"article_clean\"] = df_y[\"article\"].dropna().progress_apply(preprocess_text)\n",
    "df_y[[\"article\", \"article_clean\"]].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f051341-e088-4134-92e3-594c70fb5c10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "topic_to_id = {topic: id for id, topic in enumerate(df[\"simple_topic\"].unique())}\n",
    "id_to_topic = {id: topic for topic, id in topic_to_id.items()}\n",
    "\n",
    "df_y[\"simple_topic_id\"] = df_y[\"simple_topic\"].map(topic_to_id)\n",
    "df_y[[\"simple_topic\", \"simple_topic_id\"]].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3397d55-8d41-4f71-a769-19105af6c9df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_y = df_y.dropna(subset=[\"article_clean\"]).dropna(subset=[\"simple_topic_id\"])\n",
    "print(df_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2378195b-65f3-43fb-9937-4bd5783affb3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "BASE_DIR_NAME: str = \"experiment_results\"\n",
    "NOTEBOOK_DIR_NAME: str = \"bert_article\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bef2692-8a72-435c-b97d-40f60b34d20d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_bert_classifier(\n",
    "    df: pd.DataFrame,\n",
    "    input_col: str,\n",
    "    target_col: str,\n",
    "    sample_size: int | None = None,\n",
    "    target_col_inverse_mapping: dict | None = None,\n",
    "    max_len: int = 64,\n",
    "    bert_model_name: str = 'bert-base-uncased',\n",
    "    learning_rate: float = 0.00001,\n",
    "    batch_size: int = 32,\n",
    "    epochs: int = 3,\n",
    "):\n",
    "    title_topic_dir_name: str = (\n",
    "        f\"./{BASE_DIR_NAME}\"\n",
    "        + f\"/{NOTEBOOK_DIR_NAME}\"\n",
    "        + f\"/bert_simple\"\n",
    "        + f\"/{datetime.utcnow().strftime('%Y%m%d-%H%M')}\"\n",
    "    )\n",
    "    os.makedirs(title_topic_dir_name, exist_ok=True)\n",
    "\n",
    "    sample_size = sample_size if sample_size else len(df)\n",
    "    sample_df: pd.DataFrame = df.sample(sample_size)\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "    def encode_texts(texts):\n",
    "        return tokenizer.batch_encode_plus(\n",
    "            texts,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "\n",
    "    input_values = encode_texts(sample_df[input_col].values.tolist())\n",
    "    target_values = sample_df[target_col].values\n",
    "\n",
    "    # Convert input_ids to a numpy array\n",
    "    input_ids = np.array(input_values['input_ids'])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        input_ids,\n",
    "        target_values,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    bert = TFBertModel.from_pretrained(bert_model_name)\n",
    "\n",
    "    input_ids = Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n",
    "    bert_output = bert(input_ids)[1]\n",
    "    dropout = Dropout(0.3)(bert_output)\n",
    "    output = Dense(len(np.unique(target_values)), activation='softmax')(dropout)\n",
    "\n",
    "    model = Model(inputs=input_ids, outputs=output)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    model_history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs\n",
    "    )\n",
    "\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "\n",
    "    target_classification_report = classification_report(\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        target_names=[target_col_inverse_mapping[i] for i in sorted(set(y_test) | set(y_pred)) if i in target_col_inverse_mapping],\n",
    "    )\n",
    "\n",
    "    with open(f\"{title_topic_dir_name}/classification_report.txt\", \"w\") as f:\n",
    "        f.write(target_classification_report)\n",
    "\n",
    "    output_params = dict(\n",
    "        max_len=max_len,\n",
    "        bert_model_name=bert_model_name,\n",
    "        learning_rate=learning_rate,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "    )\n",
    "\n",
    "    with open(f\"{title_topic_dir_name}/hyperparameters.json\", \"w\") as f:\n",
    "        json.dump(output_params, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "018f005d-ee3d-4d2a-a0c3-bbde9623e8aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_bert_classifier(\n",
    "    df=df_y,\n",
    "    input_col=\"article_clean\",\n",
    "    target_col=\"simple_topic_id\",\n",
    "    target_col_inverse_mapping=id_to_topic,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cad6f666-240a-41ea-9ce3-056ade5cee6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bert_simple_model",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
