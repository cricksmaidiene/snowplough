{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cb1a6f9-b932-4348-859a-b65acb3d2b39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Preprocessor: Word Counts & Sentiments 👓\n",
    "\n",
    "This notebook adds additional columns around word count and sentiment scores, that are used in the text based EDA\n",
    "\n",
    "#### Notebook Properties\n",
    "* Upstream Notebook: `src.data_ingestion.all_the_news_v2_ingest`\n",
    "* Compute Resources: `32 GB RAM, 4 CPUs` (when not performing EDA on a sample of data)\n",
    "* Last Updated: `Nov 23 2023`\n",
    "\n",
    "#### Data\n",
    "\n",
    "| **Name** | **Type** | **Location Type** | **Description** | **Location** | \n",
    "| --- | --- | --- | --- | --- | \n",
    "| `all_the_news` | `input` | `Delta` | Read full delta dataset of `AllTheNews` | `catalog/raw/all_the_news.delta` | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ab6a4ac-0064-4e81-89e8-aa66fcd53d47",
     "showTitle": true,
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "import contextlib\n",
    "from tqdm.autonotebook import tqdm\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from deltalake import DeltaTable\n",
    "from deltalake.exceptions import TableNotFoundError\n",
    "import pyarrow as pa\n",
    "\n",
    "from src.utils.io import FileSystemHandler, partition_dataframe\n",
    "from src.utils.schemas import all_the_news_raw_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13424b40-d3a5-4808-9b0b-9140d3c088da",
     "showTitle": true,
     "title": "Settings"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "tqdm.pandas()\n",
    "\n",
    "datafs = FileSystemHandler(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bb2bdb2-84b9-41ba-ae70-068c4f1eedd1",
     "showTitle": true,
     "title": "Input Parameters"
    }
   },
   "outputs": [],
   "source": [
    "LIMIT_PARTITIONS: int | None = None\n",
    "\"\"\"An input parameter to limit the number of table partitions to read from delta. Useful to perform EDA on a sample of data.\"\"\"\n",
    "\n",
    "SHUFFLE_PARTITIONS: bool = False\n",
    "\"\"\"Whether to randomize the partitions before reading\"\"\"\n",
    "\n",
    "INPUT_TABLE: str = \"all_the_news\" \n",
    "INPUT_CATALOG: str = \"raw\"\n",
    "\n",
    "OUTPUT_TABLE: str = \"all_the_news\"\n",
    "OUTPUT_CATALOG: str =\"text_eda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e2b031f-7015-48bf-a201-b46337e72c13",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da8a4b4f-0e16-4539-8085-e567d4070fa3",
     "showTitle": true,
     "title": "Read Raw Data"
    }
   },
   "outputs": [],
   "source": [
    "atn_delta_table: DeltaTable = datafs.read_delta(\n",
    "    table=INPUT_TABLE,\n",
    "    catalog_name=INPUT_CATALOG,\n",
    "    as_pandas=False,\n",
    ")\n",
    "\n",
    "df: pd.DataFrame = datafs.read_delta_partitions(\n",
    "    delta_table=atn_delta_table,\n",
    "    N_partitions=LIMIT_PARTITIONS,\n",
    "    shuffle_partitions=SHUFFLE_PARTITIONS,\n",
    ")\n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df = df.sort_values(by=[\"date\"])\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d27283cf-cfa8-4657-8634-28f358b1bd60",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Summary Text Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a86a8780-fd5a-4a2f-8ce7-bf15d35f40c9",
     "showTitle": true,
     "title": "Character Counts"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Sum of Character Counts in Article Titles: {df.title.dropna().apply(len).sum():,}\")\n",
    "print(f\"Sum of Character Counts in Article Bodies: {df.article.dropna().apply(len).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51686e6d-d6fb-4361-83db-1af79b476d58",
     "showTitle": true,
     "title": "Word Count Processor"
    }
   },
   "outputs": [],
   "source": [
    "df[\"title_word_count\"] = df[\"title\"].dropna().apply(lambda x: len(str(x).split()))\n",
    "df[\"article_word_count\"] = df[\"article\"].dropna().progress_apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d75eb964-157d-4f85-915b-3dddf6330f68",
     "showTitle": true,
     "title": "Word Counts"
    }
   },
   "outputs": [],
   "source": [
    "title_word_count_sum: int = df[\"title_word_count\"].fillna(0).astype(\"int64\").sum()\n",
    "article_word_count_sum: int = df[\"article_word_count\"].fillna(0).astype(\"int64\").sum()\n",
    "print(f\"Word Counts Sum - Article Titles: {title_word_count_sum:,.0f}\")\n",
    "print(f\"Word Counts Sum - Article Bodies: {article_word_count_sum:,.0f}\")\n",
    "\n",
    "title_word_count_mean: int = df[\"title_word_count\"].fillna(0).astype(\"int64\").mean()\n",
    "article_word_count_mean: int = df[\"article_word_count\"].fillna(0).astype(\"int64\").mean()\n",
    "print()\n",
    "print(f\"Word Counts Mean - Article Titles: {title_word_count_mean:,.0f}\")\n",
    "print(f\"Word Counts Mean - Article Bodies: {article_word_count_mean:,.0f}\")\n",
    "\n",
    "title_word_count_med: int = df[\"title_word_count\"].fillna(0).astype(\"int64\").median()\n",
    "art_word_count_med: int = df[\"article_word_count\"].fillna(0).astype(\"int64\").median()\n",
    "print()\n",
    "print(f\"Word Counts Median - Article Titles: {title_word_count_med:,.0f}\")\n",
    "print(f\"Word Counts Median - Article Bodies: {art_word_count_med:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0ff81a6-4cfe-4867-a1c3-bf4a7de9922f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Apply Sentiment Models\n",
    "\n",
    "The following sentiment models are used through open-source packages:\n",
    "* `vaderSentiment`\n",
    "* `textblob`\n",
    "\n",
    "This is used to study news sentiments over time, by section and others as an open exploratory data analysis, and also to see differences between model scores for certain articles and why this might be the case. Biased articles might exhibit extreme positive or negative sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc4bf096-286d-42ce-b90c-539c5a37b0df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### textBlob sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0d779ff-c245-435c-b23b-a17ea7d4856a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "⚠️ The runtime of this cell is approximately 2 hours for the full volume of data\n",
    "\n",
    "> Consider speed-up by parallel processing and other methods in future. For now, processed data is stored in Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "787bc0b1-8ec0-4506-a33a-f89608d4d2c9",
     "showTitle": true,
     "title": "Assign Textblob Sentiment Scores"
    }
   },
   "outputs": [],
   "source": [
    "df[\"title_textblob_sentiment\"] = (\n",
    "    df[\"title\"].dropna().progress_apply(lambda text: TextBlob(text).sentiment.polarity)\n",
    ")\n",
    "df[\"article_textblob_sentiment\"] = (\n",
    "    df[\"article\"].dropna().progress_apply(lambda text: TextBlob(text).sentiment.polarity)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e195209f-6c99-488b-861b-ac015da4b6b2",
     "showTitle": true,
     "title": "View Textblob Data"
    }
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8aa86fb6-2640-4d69-a8df-4568b09ca914",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### vaderSentiment\n",
    "\n",
    "\n",
    "Why `VADER` (Valence Aware Dictionary & Sentiment Reasoner):\n",
    "- VADER is finely-tuned to analyze sentiments in social media text. It effectively handles the nuances and idiosyncrasies of online textual content, like emoticons, slangs, and abbreviations, which are often challenging for traditional sentiment analysis tools.\n",
    "- Unlike many sentiment analyzers that rely purely on machine learning models, VADER uses a combination of a lexicon (a list of lexical features such as words, emoji, etc., each tagged with its sentiment intensity) and a set of grammatical and syntactical rules to determine sentiment.\n",
    "- Because of its lexicon and rule-based nature, VADER does not require extensive training on large datasets\n",
    "\n",
    "`VADER` provides four scores:\n",
    "- **Positive**: Probability of the text being positive.\n",
    "- **Negative**: Probability of the text being negative.\n",
    "- **Neutral**: Probability of the text being neutral.\n",
    "- **Compound**: A normalized, weighted composite score which takes into account the other scores. This score is often used as a singular measure of sentiment for a given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd15db5c-6906-4e15-b0a1-d608c8ab46ea",
     "showTitle": true,
     "title": "VADER declarations"
    }
   },
   "outputs": [],
   "source": [
    "vader_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def assign_vader_scores(text: str) -> list[float]:\n",
    "    \"\"\"Returns vader scores as a vector to assign to pandas columns.\"\"\"\n",
    "    return_list: list = [None] * 4\n",
    "\n",
    "    with contextlib.suppress(Exception):\n",
    "        vader_dict: dict[str, float] = vader_analyzer.polarity_scores(text)\n",
    "        return_list[0] = vader_dict[\"pos\"]\n",
    "        return_list[1] = vader_dict[\"neg\"]\n",
    "        return_list[2] = vader_dict[\"neu\"]\n",
    "        return_list[3] = vader_dict[\"compound\"]\n",
    "\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e91cdb34-571f-46b1-9f1e-4c5ae2ed0379",
     "showTitle": true,
     "title": "Apply Vader Sentiments on Title"
    }
   },
   "outputs": [],
   "source": [
    "df[\n",
    "    [\n",
    "        \"vader_prob_positive_title\",\n",
    "        \"vader_prob_negative_title\",\n",
    "        \"vader_prob_neutral_title\",\n",
    "        \"vader_compound_title\",\n",
    "    ]\n",
    "] = df.progress_apply(\n",
    "    lambda row: assign_vader_scores(row.title), axis=1, result_type=\"expand\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88cedbbf-f9b4-4c76-a8b4-a1c51c0e0e02",
     "showTitle": true,
     "title": "View Results"
    }
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35a394b1-0df3-411e-8174-0c93c44ab2db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4033d22-6abf-4697-b70e-16001ce0811a",
     "showTitle": true,
     "title": "Remove Delta Table if Exists"
    }
   },
   "outputs": [],
   "source": [
    "with contextlib.suppress(TableNotFoundError):\n",
    "    \"\"\"if table already doesn't exist, then ignore\"\"\"\n",
    "    print(datafs.clear_delta(table=OUTPUT_TABLE, catalog_name=OUTPUT_CATALOG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f00f499-0694-4be5-b38a-a75b7a37f6ee",
     "showTitle": true,
     "title": "Declare Delta Schema"
    }
   },
   "outputs": [],
   "source": [
    "new_text_fields: list[pa.field] = [\n",
    "    pa.field(\"title_word_count\", pa.int64()),\n",
    "    pa.field(\"article_word_count\", pa.int64()),\n",
    "    pa.field(\"title_textblob_sentiment\", pa.float64()),\n",
    "    pa.field(\"article_textblob_sentiment\", pa.float64()),\n",
    "    pa.field(\"vader_prob_positive_title\", pa.float64()),\n",
    "    pa.field(\"vader_prob_negative_title\", pa.float64()),\n",
    "    pa.field(\"vader_prob_neutral_title\", pa.float64()),\n",
    "    pa.field(\"vader_compound_title\", pa.float64()),\n",
    "]\n",
    "\n",
    "all_the_news_text_eda_schema = all_the_news_raw_schema\n",
    "\n",
    "for new_field in new_text_fields:\n",
    "    all_the_news_text_eda_schema = all_the_news_text_eda_schema.append(new_field)\n",
    "\n",
    "all_the_news_text_eda_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10430a09-6ada-4785-ae25-00b14fa7b37b",
     "showTitle": true,
     "title": "Save Data as Partitions"
    }
   },
   "outputs": [],
   "source": [
    "df_partitions: list[pd.DataFrame] = partition_dataframe(df, N_Partitions=54)\n",
    "\n",
    "for p_df in tqdm(df_partitions):\n",
    "    datafs.write_delta(\n",
    "        dataframe=p_df,\n",
    "        table=OUTPUT_TABLE,\n",
    "        catalog_name=OUTPUT_CATALOG,\n",
    "        schema=all_the_news_text_eda_schema,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa9e8a8d-2585-4453-ba75-e58dcc85a880",
     "showTitle": true,
     "title": "Read Sample Stored Data"
    }
   },
   "outputs": [],
   "source": [
    "sample_df: pd.DataFrame = datafs.read_delta_partitions(\n",
    "    delta_table=datafs.read_delta(\n",
    "        table=OUTPUT_TABLE,\n",
    "        catalog_name=OUTPUT_CATALOG,\n",
    "    ),\n",
    "    N_partitions=1,\n",
    "    shuffle_partitions=True,\n",
    ")\n",
    "print(sample_df.info())\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3162b8e5-f4f3-43e8-b28e-effb56f97f94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "preprocessor_wc_and_sentiments",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
